This is the code used to generate data for our INTERACT 2019 paper, "Evidence Humans Provide When Explaining Data-Labeling Decisions".

Python3 and the ag (silver searcher) tool are required. Take the following steps (specified for Linux but can be translated to other operating systems) to download Visual Genome v1.4, install relevant libraries, and run the code.

1. Make sure that venv is installed on your machine. It should already be in the Python3 standard library without needing additional installation.

2. Create virtual environment (only has to be done once): =python3 -m venv env=

3. Activate virtual environment: =source env/bin/activate=

4. Install required python3 packages: =pip install -r requirements.txt=

5. Download Visual Genome v1.4 dataset into the visualgenome-1.4 directory (which will be created in the process and is required for the code; only has to be done once): =wget -P visualgenome-1.4/ -i visualgenome_reqs.txt && unzip visualgenome-1.4/\*.zip -d visualgenome-1.4=

6. Make sure that the silver searcher tool (ag) is installed (https://github.com/ggreer/the_silver_searcher). It is only used by automatedLabeling.py for generating a compact version of relationships.json to process, however this is much faster than processing relationships.json itself. Version 2.1.0 was used. The compact version of relationships.json was still too big to be uploaded.

7. If automatedLabeling.py is somehow not able to do this, run the following command to generate a compact version of relationships.json (only has to be done once): =ag -o '(predicate":[^,]+,)|(relationship_id":[^,]+, "synsets":[^,]+,)|(image_id": [^}]+)' visualgenome-1.4/relationships.json > visualgenome-1.4/extracted_relationships.txt=

8. Run script: =python3 automatedLabeling.py [arg]= or =python3 analysis.py=

9. Once finished, exit virtual environment: =deactivate=

Files:
- automatedLabeling.py: can be ran with one of the following command line arguments:
  - "test": find the best predicting rules for concepts "nightstand" and "bride".
  - "all": find the best predicting rules for all labels in the Visual Genome dataset who are in at least 100 images, using labels that appear in at least 50 images and choosing from rules that hold true for at least 10 images.
  - "userstudy-auto": find the best predicting rules for "nightstand", "old", "eat", and "intersection" in the Visual Genome dataset.
  - "userstudy-manual": similar to "userstudy-auto", but form the candidate rules using the words that user study participants most commonly used to explain whether a concept is in an image. 
- analysis.py: generate the figures used in INTERACT 2019. Histograms with x = F1 score, y = number of target concepts whose best definitions fall into that F1 score bin.
- data/
  - study1_data.csv: responses from the user study, used to yield potential definitions of the four target concepts ("nightstand", "crossroad"/"intersection", "eating"/"eat", "old") and evaluated in Visual Genome.
  - comparisons.csv: compares definitions generated for the four target concepts from the results of the user study (up to 5 most commonly used words in responses)  and simulating in Visual Genome (up to 5 co-occurring labels with the highest F1 scores when used as definitions for the target concept). 
  - all_F1_defs.csv: lists the definition with the highest F1 score for each target concept that appears in Visual Genome more than 100 times (using labels that appear in at least 50 images and choosing from rules that hold true for at least 10 images). Definitions were generated from co-occurring labels with the highest F1 scores. No definitions could be generated for two labels, "concentrate" and "other". Sorted in decreasing order of F1 score. This is a file combined from all the data files generated by =python3 automatedLabeling.py all=.
- figs/: figures used for the INTERACT 2019 paper, generated by analysis.py.

This work uses NLTK:
=Bird, Steven, Edward Loper and Ewan Klein (2009).
Natural Language Processing with Python.  O'Reilly Media Inc.=
